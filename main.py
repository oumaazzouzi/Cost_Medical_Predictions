# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-DMSEO4j0xXCoHYLCG_rK3DL8TtpCXu7
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

"""# **Visualisation des donnees**"""

df=pd.read_csv('insurance.csv')
X=df[['age','sex','bmi','children','smoker','region']]
y=df['charges']
df.head()
df.columns
#Pas de valeurs manquantes
df.isnull().sum()
X.head()
df.info()
print(df.describe())  # Voir min, max, moyenne des valeurs cibles

"""# **Visualisation de la corrélation entre les variables**"""

import seaborn as sns

correlation_matrix = df.select_dtypes(include=np.number).corr() # Select only numeric columns
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Matrice de Corrélation')
plt.show()

df.shape

import seaborn as sns
import matplotlib.pyplot as plt

features = ['age', 'bmi', 'children']  # Liste des features à analyser
for feature in features:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df[feature], y=df['charges'])
    plt.title(f"Relation entre {feature} et charges")
    plt.xlabel(feature)
    plt.ylabel('charges')
    plt.show()

"""# **Détection des valeurs aberrantes**"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Sélectionner les colonnes numériques
selected_columns = df.select_dtypes(include=[np.number]).columns

# Boucle pour chaque colonne numérique
for col in selected_columns:
    # Boxplot
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot de {col}")
    plt.show()

    # Nuage de points pour la relation avec 'charges'
    plt.figure(figsize=(6, 4))
    plt.scatter(x=df[col], y=df['charges'], alpha=0.5)
    plt.xlabel(col)
    plt.ylabel("Charges")
    plt.title(f"Relation entre {col} et les charges")
    plt.show()

#autre méthode
upper_lim=df['charges'].quantile(0.95)
#5% des valeurs les plus élevées de la colonne charges sont considérées comme potentiellement aberrantes.
df_outliers=df[df['charges']>upper_lim]
#Les valeurs
df_outliers

def detect_outliers(data):
  outliers=[]
  threshold=3
  mean=np.mean(data)
  std=np.std(data)
  for i in data:
    z_score=(i-mean)/std
    if np.abs(z_score)>threshold:
      outliers.append(i)

  return outliers

print(f"La listes des outliers dans : charges{detect_outliers(df['charges'])}")
print(f"La listes des outliers dans : bmi{detect_outliers(df['bmi'])}")

print(f"La listes des outliers dans : charges{detect_outliers(df['charges'])}")
print(f"La listes des outliers dans : bmi{detect_outliers(df['bmi'])}")
df['charges']=np.log1p(df['charges'])
df['bmi']=np.log1p(df['bmi'])
print(df['charges'])
print(df['bmi'])

df.info()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error
print (X.columns)

X.head()

"""# **Encodage des variables categorielles**"""

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df = pd.get_dummies(df, drop_first=True)  # Évite la redondance

"""# **Model de regression linéaire**"""

#regression linéaire
from sklearn.linear_model import LinearRegression

X=df[["age","bmi","smoker_yes"]]
Y=df["charges"]

model1 = LinearRegression()
model1.fit(X,Y)

#prediction
y_pred = model1.predict(X)

#tracage de la courbe
plt.scatter(Y, y_pred, color="blue", alpha=0.5)
plt.xlabel("Valeurs réelles (log_charges)")
plt.ylabel("Valeurs prédites")
plt.title("Prédictions vs Valeurs réelles")
plt.show()

from sklearn.metrics import mean_squared_error, r2_score

#calcul des erreurs
mse = mean_squared_error(Y,y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(Y,y_pred)

print(f"MSE = {mse}")
print(f"RMSE = {rmse}")
print(f"R2 = {r2}")

X = df.drop('charges', axis=1)  # ou celle que tu veux prédire
y = df['charges']

# Puis
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model1 = DecisionTreeRegressor(max_depth=4, random_state=42)
model1.fit(X_train, y_train)

"""# **Arbre de decision**"""

from sklearn.tree import DecisionTreeRegressor, export_text
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% for training, 20% for testing
model1=DecisionTreeRegressor(max_depth=4, random_state=42)

model1.fit(X_train, y_train)
y_pred = model1.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae=mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("-------------DecisionTreeRegressor-----------------")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Mean Absolute Error: {mae:.2f}")
print(f"R-squared: {r2}")

"""# **Visualisation de l'arbre**"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
# Visualizing decision tree
plt.figure(figsize=(20, 10))
plot_tree(
    model1,
    feature_names=X_train.columns,
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Decision Tree Structure")
plt.show()

!pip install xgboost

"""# **Model XGBOOST**"""

import xgboost as xgb
from xgboost import XGBRegressor

# Initialisation du modèle
model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)

# Entraînement
model.fit(X_train, y_train)

# Prédictions
y_pred = model.predict(X_test)

#Graphe de regression pour le model
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.7, color='blue')
plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color='red', linestyle='--')
plt.xlabel("Valeurs Réelles")
plt.ylabel("Prédictions")
plt.title("Comparaison Prédictions vs Réelles")
plt.legend(["Modèle", "45° Line"])
plt.show()